{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Boosting Machine (XGBoost) - With Tuned Hyperparameters\n",
    "### *Exploring the association between neoantigen-related variables and immune scores*\n",
    "This notebook is the continuation of the `xgboost.ipynb` notebook, which details hyperparameter tuning grid search with cross-validation to model interactions of select X features on a subset of Y labels (based on the PCA work done by Caitlin from GB team). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Package and Raw Data Loading**\n",
    "First, import necessary packages and load in the raw data table into `pandas` dataFrame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "\n",
    "from warnings import simplefilter, filterwarnings\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "filterwarnings(\"ignore\", category=UserWarning)\n",
    "pd.set_option('display.max_columns', None)\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the cleaned-up dataset wrangled from MH's latest work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in latest data\n",
    "# use the 202409_new_excludedIHC_batch-duplicate-removed.tsv\n",
    "df = pd.read_csv(\"../input-data/SA/202409_new_excludedIHC_batch-duplicate-removed.tsv\",sep=\"\\t\")\n",
    "print(df.shape)\n",
    "\n",
    "#print row 107 and 226\n",
    "df.iloc[[107,226,555,872], :25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the 29 Cibersort scores, leaving only 3\n",
    "df = df.drop(columns=['Bindea_full', 'Expanded_IFNg', \n",
    "        'C_Bcellsmemory','C_Plasmacells','C_TcellsCD8','C_TcellsCD4naive',\n",
    "         'C_TcellsCD4memoryactivated','C_Tcellsfollicularhelper',\n",
    "         'C_Tcellsregulatory(Tregs)','C_Tcellsgammadelta','C_NKcellsresting',\n",
    "         'C_NKcellsactivated', 'C_Monocytes', 'C_MacrophagesM0',\n",
    "         'C_MacrophagesM1','C_Dendriticcellsresting',\n",
    "         'C_Dendriticcellsactivated', 'C_Mastcellsresting',\n",
    "         'C_Mastcellsactivated','C_Eosinophils', 'C_Neutrophils', 'S_PAM100HRD'])\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Preprocessing**\n",
    "\n",
    "Decide all the clinical variables and neoantigen-related variables to keep in the X matrix (features).\n",
    "\n",
    "1. `Subtype` column has already been encoded categorically by `HR_status` and `HER_status` columns so these two columns can be dropped. ***UPDATE: due to their lesser importance during the default XGBoost modeling, `PAM50` column was dropped as well.***\n",
    "\n",
    "2.  `AgeGroup` is just a binned information of `Age` column so it is dropped as it is redundant.\n",
    "\n",
    "3. Drop `FusionNeo_bestScore`, `FusionTransscript_Count`, `Fusion_T2NeoRate` columns as well as the `SNVindelNeo_IC50` and `SNVindelNeo_IC50Percentile` columns for now to reduce complexity.\n",
    "\n",
    "4. Drop `Batch` column.\n",
    "\n",
    "> **UPDATE 1: Exclude `TotalNeo_Count`, and include `Fusion_T2NeoRate` and `SNVindelNeo_IC50` columns. Also, rename `Fusion_T2NeoRate` to `FN/FT_Ratio`.**\n",
    "\n",
    "> **UPDATE 2: put back `FusionNeo_bestScore` into the X variable set and rename it into `FusionNeo_bestIC50`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop all NaN for now and set col 'ID' as index\n",
    "dfd = df.drop(columns = ['Batch', 'Stage', 'PAM50', 'HR_status', 'HER_status', 'AgeGroup', 'TotalNeo_Count', 'FusionTransscript_Count', 'SNVindelNeo_IC50Percentile']).dropna().set_index('ID')\n",
    "print(dfd.shape)\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column `Fusion_T2NeoRate` to `FN/FT_Ratio` and `FusionNeo_bestScore` to `FusionNeo_bestIC50`\n",
    "dfd.rename(columns={'Fusion_T2NeoRate': 'FN/FT_Ratio'}, inplace=True)\n",
    "dfd.rename(columns={'FusionNeo_bestScore': 'FusionNeo_bestIC50'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** Check to make sure there is no duplicated index rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfd.index[dfd.index.duplicated()].unique())\n",
    "rows_dupe = list(dfd.index[dfd.index.duplicated()].unique())\n",
    "rows_dupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can encode categorical variables and integer variables accordingly so they are more amenable to machine learning algorithms. \n",
    "\n",
    "Check data types of the resulting cleaned up dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check original df's dtypes\n",
    "pd.set_option('display.max_rows', None)\n",
    "dfd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the `object` columns of `Subtype` and `FN/FT_Ratio` into appropriate types. Change `Age`, `TumorGrade`, and `IMPRES` into `int64` as well as all `*_Count` columns because they are discrete variables. Change the `FN/FT_Ratio` into `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd['Subtype'] = dfd['Subtype'].astype('category')\n",
    "dfd['Age'] = dfd['Age'].astype('int64')\n",
    "dfd['TumorGrade'] = dfd['TumorGrade'].astype('int64')\n",
    "dfd['IMPRES'] = dfd['IMPRES'].astype('int64')\n",
    "dfd['FusionNeo_Count'] = dfd['FusionNeo_Count'].astype('int64')\n",
    "dfd['SNVindelNeo_Count'] = dfd['SNVindelNeo_Count'].astype('int64')\n",
    "dfd['FN/FT_Ratio'] = dfd['FN/FT_Ratio'].astype('float64')\n",
    "\n",
    "print(dfd.dtypes)\n",
    "pd.set_option('display.max_rows', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Feature_Engine's `OneHotEncoder()` to create a `k` dummy variable set for `Subtype`.\n",
    "\n",
    "**NOTE**: The encoded columns will be appended at the end of the dataFrame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    variables=['Subtype'],\n",
    "    drop_last=False)\n",
    "\n",
    "encoder.fit(dfd)\n",
    "dfd_ = encoder.transform(dfd)\n",
    "dfd_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the encoded columns to shift\n",
    "enc_cols = ['Subtype_HR+/HER2-', 'Subtype_HR+/HER2+', 'Subtype_TNBC', 'Subtype_HR-/HER2+']\n",
    "\n",
    "# Drop the specified columns and store them\n",
    "encoded_df = dfd_[enc_cols]\n",
    "dfenc = dfd.drop(columns=['Subtype'])\n",
    "\n",
    "# Specify the index where you want to reinsert the columns\n",
    "insert_index = 0  # This will insert at the first column\n",
    "\n",
    "# Reinsert the columns\n",
    "for i, col in enumerate(encoded_df.columns):\n",
    "    dfenc.insert(insert_index + i, col, encoded_df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the categorically-encoded dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfenc.shape)\n",
    "dfenc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below is the original, unencoded dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfd.shape)\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Subsetting Y Labels for XGBoost**\n",
    "\n",
    "In the previous exploration, many of the immune scores (Y targets/labels) might not really show much relationship with fusion neoantigen variables so they may not be as informative. We decided to use Caitlin's finding and subset the Y labels into several clinically meaningful groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the unencoded categorical dataframe (dfd) and drop the Subtype categorical column\n",
    "df_dcat = dfd.drop(columns=['Subtype'])\n",
    "print(df_dcat.shape)\n",
    "df_dcat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First list all the clinical variables that would be the X feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = ['Subtype_HR+/HER2-', 'Subtype_HR+/HER2+', 'Subtype_TNBC', 'Subtype_HR-/HER2+', 'Age', 'TumorGrade', 'TumourSize', 'FusionNeo_Count', 'FusionNeo_bestIC50', 'FN/FT_Ratio', 'SNVindelNeo_Count', 'SNVindelNeo_IC50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_nocat = ['Age', 'TumorGrade', 'TumourSize', 'FusionNeo_Count', 'FusionNeo_bestIC50', 'FN/FT_Ratio', 'SNVindelNeo_Count', 'SNVindelNeo_IC50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the Y variable set\n",
    "Y_labels_all = [col for col in dfd.drop(columns=['Subtype']).columns if col not in X_features]\n",
    "print(Y_labels_all[:5])\n",
    "len(Y_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the tsv containing the groupings of the different immune scores\n",
    "df_imscores = pd.read_csv('../input-data/SA/immune_score_groupings.tsv', sep='\\t')\n",
    "df_imscores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imscores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert each column into a Series and drop NA\n",
    "# Create a dictionary to store the Series\n",
    "imscore_series_dict = {}\n",
    "\n",
    "# Iterate through each column in the DataFrame\n",
    "for column in df_imscores.columns:\n",
    "    # Convert the column to a Series, drop NaN values, and store in the dictionary\n",
    "    imscore_series_dict[column] = df_imscores[column].dropna().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Spearman Correlation Heatmap**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on with XGBoost, we can plot a Spearman correlation matrix between the clinical variables (X features) and the different groupings of immune score variables (Y labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, draw a matrix on the full cleaned up dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replot heatmap on transformed data\n",
    "corr_df = df_dcat.corr(method='spearman')\n",
    "corr_df = corr_df.round(2)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(46, 36))\n",
    "\n",
    "# Create the correlation matrix and represent it as a heatmap.\n",
    "hm = sns.heatmap(corr_df, annot = False, cmap = 'RdBu_r', square = True, linewidths=0.5, center=0, mask=mask, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# Get current labels\n",
    "ylabels = hm.get_yticklabels()\n",
    "xlabels = hm.get_xticklabels()\n",
    "\n",
    "# Hide the first y-axis label and the last x-axis label\n",
    "ylabels[0].set_visible(False)\n",
    "xlabels[-1].set_visible(False)\n",
    "\n",
    "# Rotate and align the tick labels\n",
    "plt.setp(xlabels, rotation=45, ha='right')\n",
    "\n",
    "# Define columns to highlight and their colors\n",
    "highlight_cols = {\n",
    "    \"FN/FT_Ratio\": \"crimson\",\n",
    "    \"FusionNeo_Count\": \"olive\",\n",
    "    \"SNVindelNeo_Count\": \"darkviolet\"\n",
    "}\n",
    "\n",
    "# Change color of specific x-axis and y-axis labels\n",
    "for label in xlabels:\n",
    "    if label.get_text() in highlight_cols:\n",
    "        label.set_color(highlight_cols[label.get_text()])\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "for label in ylabels:\n",
    "    if label.get_text() in highlight_cols:\n",
    "        label.set_color(highlight_cols[label.get_text()])\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "# Removes all ticks\n",
    "hm.tick_params(left=False, bottom=False)\n",
    "\n",
    "hm.set_title('Dataframe Correlation Heatmap', fontsize=30, x=0.45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, draw the same on the subset Y labels grouped as `activator_T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list from the dict\n",
    "activator_t = imscore_series_dict['activator_T']\n",
    "suppressor_t = imscore_series_dict['suppressor_T']\n",
    "best_prog = imscore_series_dict['HR<1_best_10_prog']\n",
    "worst_prog = imscore_series_dict['HR>1_worst_10_prog']\n",
    "\n",
    "# combine these with the X feature set (unique only using set())\n",
    "merged_cols = X_features_nocat + activator_t + suppressor_t + best_prog + worst_prog\n",
    "merged_cols = list(set(merged_cols))\n",
    "print(f\"Total number of elements in merged_cols (unsorted): {len(merged_cols)}\")\n",
    "\n",
    "# there are repeated immune scores (at least in between two groups, can be more than two groups) so get a list of them first\n",
    "from itertools import combinations\n",
    "# list of all the sets\n",
    "all_sets = [set(activator_t), set(suppressor_t), set(best_prog), set(worst_prog)]\n",
    "\n",
    "# Get all possible combinations of 2 sets\n",
    "set_combo = combinations(all_sets, 2)\n",
    "\n",
    "# Find the union of all set combinations\n",
    "union_of_combo = list(set.union(*[set.intersection(c1, c2) for c1, c2 in set_combo]))\n",
    "\n",
    "print(f\"Elements that overlap between at least two sets: {union_of_combo}\")\n",
    "\n",
    "# rearrange the list element order based on another list\n",
    "merged_cols = [x for x in X_features_nocat] + union_of_combo + [x for x in activator_t if x not in union_of_combo] + [x for x in suppressor_t if x not in union_of_combo] + [x for x in best_prog if x not in union_of_combo] + [x for x in worst_prog if x not in union_of_combo]\n",
    "\n",
    "print(f\"Total number of elements in merged_cols (sorted by original X feature order and groups): {len(merged_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now subset the dcat data\n",
    "df_dcat_ss = df_dcat[merged_cols]\n",
    "df_dcat_ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replot heatmap on transformed data\n",
    "corr_df = df_dcat_ss.corr(method='spearman')\n",
    "corr_df = corr_df.round(2)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(46, 36))\n",
    "\n",
    "# Create the correlation matrix and represent it as a heatmap.\n",
    "hm = sns.heatmap(corr_df, annot = False, cmap = 'RdBu_r', square = True, linewidths=0.5, center=0, mask=mask, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# Get current labels\n",
    "ylabels = hm.get_yticklabels()\n",
    "xlabels = hm.get_xticklabels()\n",
    "\n",
    "# Hide the first y-axis label and the last x-axis label\n",
    "ylabels[0].set_visible(False)\n",
    "xlabels[-1].set_visible(False)\n",
    "\n",
    "# Rotate and align the tick labels\n",
    "plt.setp(xlabels, rotation=45, ha='right')\n",
    "\n",
    "# Define columns to highlight and their colors\n",
    "highlight_cols = {\n",
    "    \"FN/FT_Ratio\": \"crimson\",\n",
    "    \"FusionNeo_Count\": \"olive\",\n",
    "    \"SNVindelNeo_Count\": \"darkviolet\"\n",
    "}\n",
    "\n",
    "# Change color of specific x-axis and y-axis labels\n",
    "for label in xlabels:\n",
    "    if label.get_text() in highlight_cols:\n",
    "        label.set_color(highlight_cols[label.get_text()])\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "for label in ylabels:\n",
    "    if label.get_text() in highlight_cols:\n",
    "        label.set_color(highlight_cols[label.get_text()])\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "# Removes all ticks\n",
    "hm.tick_params(left=False, bottom=False)\n",
    "\n",
    "hm.set_title('Dataframe Correlation Heatmap', fontsize=30, x=0.45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Split Dataset with `train_test_split`**\n",
    "\n",
    "Split the dataset before modeling to avoid information leakage, then preprocess the data through the set up Pipeline before XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset X features; use the list generated before\n",
    "X = dfenc[X_features]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grab the Y targets (do this as a whole, but we will train on each column individually later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the Y variable set\n",
    "Y = dfenc[Y_labels_all]\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform train test split on the X and Y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don't want to transform all the X columns (because some of them are discrete numerical data and some of them are one-hot encoded categorical variables), we need to specify the columns to transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create a Data Transformation Pipeline from `feature_engine` Package**\n",
    "First, the pipeline will apply the Yeo-Johnson transformation on the split datasets on select X features and all Y labels, and scale them using `StandardScaler` (but wrapped within `feature_engine`'s wrapper) on select X features and all Y labels.\n",
    "\n",
    "This pipeline would enable easy inverse transform steps for both X and Y datasets later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars_to_transform = ['TumourSize', 'FusionNeo_Count', 'FusionNeo_bestIC50', 'FN/FT_Ratio', 'SNVindelNeo_Count', 'SNVindelNeo_IC50']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do for X datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.pipeline import Pipeline\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# select variables to scale\n",
    "scale_cols_X = X_train.columns.tolist()\n",
    "scale_cols_X = [col for col in scale_cols_X if col not in ['Age', 'TumorGrade', 'Subtype_HR+/HER2-', 'Subtype_HR+/HER2+', 'Subtype_TNBC', 'Subtype_HR-/HER2+']]\n",
    "\n",
    "# Create the pipeline\n",
    "preprocess_pipeline_X = Pipeline([\n",
    "    ('yeo_johnson', YeoJohnsonTransformer(variables=X_vars_to_transform)),\n",
    "    ('scaler', SklearnTransformerWrapper(transformer = StandardScaler(), variables = scale_cols_X))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "preprocess_pipeline_X.fit(X_train)\n",
    "\n",
    "# Transform the training data\n",
    "X_train_yjs = preprocess_pipeline_X.transform(X_train)\n",
    "# Transform the test data\n",
    "X_test_yjs = preprocess_pipeline_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_yjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables to scale\n",
    "scale_cols_Y = Y_train.columns.tolist()\n",
    "\n",
    "# Create the pipeline\n",
    "preprocess_pipeline_Y = Pipeline([\n",
    "    ('yeo_johnson', YeoJohnsonTransformer()),\n",
    "    ('scaler', SklearnTransformerWrapper(transformer = StandardScaler(), variables = scale_cols_Y))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "preprocess_pipeline_Y.fit(Y_train)\n",
    "\n",
    "# Transform the training data\n",
    "Y_train_yjs = preprocess_pipeline_Y.transform(Y_train)\n",
    "# Transform the test data\n",
    "Y_test_yjs = preprocess_pipeline_Y.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_yjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XGBoost Learning**\n",
    "\n",
    "Time to test XGBoost. Select `S_Buck14_score` column as the first target/label (`y`) variable first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "y_target = 'S_Buck14_score'\n",
    "y_train_tg = Y_train_yjs[y_target]\n",
    "y_test_tg = Y_test_yjs[y_target]\n",
    "\n",
    "model_xgb = XGBRegressor(n_estimators=150, random_state=42)\n",
    "\n",
    "# fit\n",
    "model_xgb.fit(X_train_yjs, y_train_tg)\n",
    "\n",
    "# predict\n",
    "y_train_transformed_pred = model_xgb.predict(X_train_yjs)\n",
    "y_test_transformed_pred = model_xgb.predict(X_test_yjs)\n",
    "\n",
    "# Create a DataFrame with the same columns as the original y used in fit\n",
    "y_train_dummy_df = pd.DataFrame(0, index=X_train_yjs.index, columns=Y_train_yjs.columns)\n",
    "y_train_dummy_df[y_target] = y_train_transformed_pred\n",
    "\n",
    "y_test_dummy_df = pd.DataFrame(0, index=X_test_yjs.index, columns=Y_test_yjs.columns)\n",
    "y_test_dummy_df[y_target] = y_test_transformed_pred\n",
    "\n",
    "# apply inverse transform\n",
    "y_train_dummy_df_inv = preprocess_pipeline_Y.inverse_transform(y_train_dummy_df)\n",
    "y_test_dummy_df_inv = preprocess_pipeline_Y.inverse_transform(y_test_dummy_df)\n",
    "\n",
    "# Extract the relevant target column\n",
    "y_train_pred = y_train_dummy_df_inv[y_target].to_numpy()\n",
    "y_test_pred = y_test_dummy_df_inv[y_target].to_numpy()\n",
    "\n",
    "# Evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2 = r2_score(Y_train[y_target], y_train_pred)\n",
    "test_r2 = r2_score(Y_test[y_target], y_test_pred)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(Y_train[y_target], y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(Y_test[y_target], y_test_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(Y_train[y_target], y_train_pred)\n",
    "test_mae = mean_absolute_error(Y_test[y_target], y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Model Performance:\")\n",
    "print(f\"{'Metric':<10} {'Train':<10} {'Test':<10}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'R2':<10} {train_r2:<10.4f} {test_r2:<10.4f}\")\n",
    "print(f\"{'RMSE':<10} {train_rmse:<10.4f} {test_rmse:<10.4f}\")\n",
    "print(f\"{'MAE':<10} {train_mae:<10.4f} {test_mae:<10.4f}\")\n",
    "\n",
    "# Plot actual vs predicted\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.scatter(Y_train[y_target], y_train_pred, alpha=0.5)\n",
    "ax1.plot([Y_train[y_target].min(), Y_train[y_target].max()], [Y_train[y_target].min(), Y_train[y_target].max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual')\n",
    "ax1.set_ylabel('Predicted')\n",
    "ax1.set_title('Train Set')\n",
    "\n",
    "ax2.scatter(Y_test[y_target], y_test_pred, alpha=0.5)\n",
    "ax2.plot([Y_test[y_target].min(), Y_test[y_target].max()], [Y_test[y_target].min(), Y_test[y_target].max()], 'r--', lw=2)\n",
    "ax2.set_xlabel('Actual')\n",
    "ax2.set_ylabel('Predicted')\n",
    "ax2.set_title('Test Set')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "fig, ax = plt.subplots(figsize=(18, 16), dpi=300)\n",
    "plot_importance(model_xgb, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "import shap\n",
    "# Create the SHAP explainer\n",
    "explainer = shap.TreeExplainer(model_xgb)\n",
    "\n",
    "# run explanation object on X_test dataset because we don't want to learn what the model learned from the X_train data, but to see features that would influence predictions on new data\n",
    "shap_values = explainer(X_test_yjs)\n",
    "\n",
    "plt.figure(figsize=(18, 16)) \n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Perform clustering\n",
    "clust = shap.utils.hclust(X_test_yjs, y_test_tg)\n",
    "# Create the bar plot with clustering\n",
    "plt.figure(figsize=(18, 16)) \n",
    "shap.plots.bar(shap_values, clustering=clust, clustering_cutoff=1, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Iterative Learning over all Y labels**\n",
    "\n",
    "The learning using XGBoost above was done on just one Y label, which is the `ESTIMATE` column. Let's put these into a set of functions so we can run this process iteratively on all Y columns we have set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the steps above as a function\n",
    "import os \n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "class YTargetMetrics:\n",
    "    def __init__(self, target_name, train_r2, test_r2, train_rmse, test_rmse, train_mae, test_mae):\n",
    "        self.target_name = target_name\n",
    "        self.train_r2 = train_r2\n",
    "        self.test_r2 = test_r2\n",
    "        self.train_rmse = train_rmse\n",
    "        self.test_rmse = test_rmse\n",
    "        self.train_mae = train_mae\n",
    "        self.test_mae = test_mae\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"Model Performance for {self.target_name}:\n",
    "{'Metric':<10} {'Train':<10} {'Test':<10}\n",
    "{'-' * 30}\n",
    "{'R2':<10} {self.train_r2:<10.4f} {self.test_r2:<10.4f}\n",
    "{'RMSE':<10} {self.train_rmse:<10.4f} {self.test_rmse:<10.4f}\n",
    "{'MAE':<10} {self.train_mae:<10.4f} {self.test_mae:<10.4f}\"\"\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'target_name': self.target_name,\n",
    "            'train_r2': self.train_r2,\n",
    "            'test_r2': self.test_r2,\n",
    "            'train_rmse': self.train_rmse,\n",
    "            'test_rmse': self.test_rmse,\n",
    "            'train_mae': self.train_mae,\n",
    "            'test_mae': self.test_mae\n",
    "        }\n",
    "\n",
    "def plot_learning_curves(estimator, X, y, target, cv=5, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=train_sizes,\n",
    "        scoring='neg_mean_squared_error')\n",
    "    \n",
    "    train_scores_mean = -train_scores.mean(axis=1)\n",
    "    train_scores_std = train_scores.std(axis=1)\n",
    "    test_scores_mean = -test_scores.mean(axis=1)\n",
    "    test_scores_std = test_scores.std(axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=300)\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    # Create 'plots' directory if it doesn't exist\n",
    "    os.makedirs(f'plots/{target}', exist_ok=True)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(f'plots/{target}/{target}-xgb-def-model-learning-curve.png')\n",
    "    plt.close()\n",
    "\n",
    "def run_xgboost_model(y_target, Y_train, Y_test, X_train_transformed, X_test_transformed, Y_train_transformed, Y_test_transformed, preprocess_pipeline_Y):\n",
    "    # assign untransformed, raw target data\n",
    "    raw_y_train = Y_train[y_target]\n",
    "    raw_y_test = Y_test[y_target]\n",
    "\n",
    "    model_instance = XGBRegressor(n_estimators=150, random_state=42)\n",
    "\n",
    "    # fit\n",
    "    model_instance.fit(X_train_transformed, Y_train_transformed[y_target])\n",
    "    \n",
    "    # predict\n",
    "    y_train_pred_transformed = model_instance.predict(X_train_transformed)\n",
    "    y_test_pred_transformed = model_instance.predict(X_test_transformed)\n",
    "\n",
    "    # Create a DataFrame with the same columns as the original y used in fit\n",
    "    dummy_train_y = pd.DataFrame(0, index=X_train_transformed.index, columns=Y_train_transformed.columns)\n",
    "    dummy_train_y[y_target] = y_train_pred_transformed\n",
    "\n",
    "    dummy_test_y = pd.DataFrame(0, index=X_test_transformed.index, columns=Y_test_transformed.columns)\n",
    "    dummy_test_y[y_target] = y_test_pred_transformed\n",
    "\n",
    "    # apply inverse transform\n",
    "    dummy_train_y_inv = preprocess_pipeline_Y.inverse_transform(dummy_train_y)\n",
    "    dummy_test_y_inv = preprocess_pipeline_Y.inverse_transform(dummy_test_y)\n",
    "\n",
    "    # Extract the relevant target column\n",
    "    y_train_pred = dummy_train_y_inv[y_target].to_numpy()\n",
    "    y_test_pred = dummy_test_y_inv[y_target].to_numpy()\n",
    "\n",
    "    # Evaluate model\n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(raw_y_train, y_train_pred)\n",
    "    test_r2 = r2_score(raw_y_test, y_test_pred)\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(raw_y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(raw_y_test, y_test_pred))\n",
    "\n",
    "    train_mae = mean_absolute_error(raw_y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(raw_y_test, y_test_pred)\n",
    "\n",
    "    # Plot learning curves\n",
    "    plot_learning_curves(model_instance, X_train_transformed, Y_train_transformed[y_target], y_target)\n",
    "\n",
    "    # Print results\n",
    "    # print(\"Model Performance:\")\n",
    "    # print(f\"{'Metric':<10} {'Train':<10} {'Test':<10}\")\n",
    "    # print(\"-\" * 30)\n",
    "    # print(f\"{'R2':<10} {train_r2:<10.4f} {test_r2:<10.4f}\")\n",
    "    # print(f\"{'RMSE':<10} {train_rmse:<10.4f} {test_rmse:<10.4f}\")\n",
    "    # print(f\"{'MAE':<10} {train_mae:<10.4f} {test_mae:<10.4f}\")\n",
    "\n",
    "    # Plot actual vs predicted\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), dpi=300)\n",
    "\n",
    "    ax1.scatter(raw_y_train, y_train_pred, alpha=0.5)\n",
    "    ax1.plot([raw_y_train.min(), raw_y_train.max()], [raw_y_train.min(), raw_y_train.max()], 'r--', lw=2)\n",
    "    ax1.set_xlabel('Actual')\n",
    "    ax1.set_ylabel('Predicted')\n",
    "    ax1.set_title('Training Set')\n",
    "\n",
    "    ax2.scatter(raw_y_test, y_test_pred, alpha=0.5)\n",
    "    ax2.plot([raw_y_test.min(), raw_y_test.max()], [raw_y_test.min(), raw_y_test.max()], 'r--', lw=2)\n",
    "    ax2.set_xlabel('Actual')\n",
    "    ax2.set_ylabel('Predicted')\n",
    "    ax2.set_title('Testing Set')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create 'plots' directory if it doesn't exist\n",
    "    os.makedirs(f'plots/{y_target}', exist_ok=True)\n",
    "\n",
    "    plt.savefig(f'plots/{y_target}/{y_target}-xgb-def-model-performance-comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Feature importance\n",
    "    _, ax = plt.subplots(figsize=(18, 16), dpi=300)\n",
    "    plot_importance(model_instance, ax=ax)\n",
    "\n",
    "    plt.savefig(f\"plots/{y_target}/{y_target}-xgb-def-model-test-set-feature-importance.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Create the SHAP explainer\n",
    "    explainer = shap.TreeExplainer(model_instance)\n",
    "\n",
    "    # run explanation object on X_test dataset because we don't want to learn what the model learned from the X_train data, but to see features that would influence predictions on new data\n",
    "    shap_values = explainer(X_test_transformed)\n",
    "\n",
    "    plt.figure(figsize=(18, 16), dpi=300) \n",
    "    # Summary plot\n",
    "    shap.summary_plot(shap_values, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/{y_target}/{y_target}-xgb-def-model-test-set-SHAP-beeswarm.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Perform clustering\n",
    "    clust = shap.utils.hclust(X_test_transformed, Y_test_transformed[y_target])\n",
    "    # Create the bar plot with clustering\n",
    "    plt.figure(figsize=(18, 16)) \n",
    "    shap.plots.bar(shap_values, clustering=clust, clustering_cutoff=1, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/{y_target}/{y_target}-xgb-def-model-test-set-SHAP-summary.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Model training and evaluation for {y_target} completed.\")\n",
    "\n",
    "    # Instead of returning a tuple, return a YTargetMetrics object\n",
    "    return YTargetMetrics(y_target, train_r2, test_r2, train_rmse, test_rmse, train_mae, test_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Grid Search: Using `GridSearchCV` for Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "class YTargetMetrics:\n",
    "    def __init__(self, target_name, test_mae, test_rmse, test_r2):\n",
    "        self.target_name = target_name\n",
    "        self.test_mae = test_mae\n",
    "        self.test_rmse = test_rmse\n",
    "        self.test_r2 = test_r2\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"Model Performance for {self.target_name}:\n",
    "{'Metric':<10} {'Test':<10}\n",
    "{'-' * 20}\n",
    "{'MAE':<10} {self.test_mae:<10.4f}\n",
    "{'RMSE':<10} {self.test_rmse:<10.4f}\n",
    "{'R2':<10} {self.test_r2:<10.4f}\"\"\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'target_name': self.target_name,\n",
    "            'test_mae': self.test_mae,\n",
    "            'test_rmse': self.test_rmse,\n",
    "            'test_r2': self.test_r2\n",
    "        }\n",
    "\n",
    "\n",
    "def run_grid_search_cv(model_instance, target, search_space, X_train_transformed, Y_train_transformed, output_path):\n",
    "    # assign y target column\n",
    "    y_train_target = Y_train_transformed[target]\n",
    "    \n",
    "    # Set up grid search\n",
    "    grid_search = GridSearchCV(\n",
    "    estimator=model_instance,\n",
    "    param_grid=search_space,\n",
    "    cv=5,\n",
    "    refit='r2',\n",
    "    scoring=['r2', 'neg_root_mean_squared_error'],\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Running grid search on {target} column...\")\n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train_transformed, y_train_target)\n",
    "\n",
    "    # Get best parameters and model\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_metric = grid_search.best_score_\n",
    "\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    print(\"Best score:\", best_metric)\n",
    "\n",
    "    # get CV results into a csv file\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    cv_results = cv_results.sort_values(by='rank_test_r2')\n",
    "    cv_results.to_csv(f'{output_path}/grid_search_results_{target}_score-{best_metric}.csv', index=False)\n",
    "    return best_model, best_params, best_metric\n",
    "\n",
    "def predict_with_best_xgboost_model(best_model, target, X_test_transformed, Y_test_transformed, Y_test, preprocess_pipeline_Y):\n",
    "    # Make predictions on the test set using the best model\n",
    "    y_pred_transformed = best_model.predict(X_test_transformed)\n",
    "\n",
    "    # Create a DataFrame with the same columns as the original y used in preprocess_pipeline to reverse transformation\n",
    "    dummy_df = pd.DataFrame(0, index=X_test_transformed.index, columns=Y_test_transformed.columns)\n",
    "    dummy_df[target] = y_pred_transformed\n",
    "\n",
    "    # apply inverse transform\n",
    "    dummy_df_inv = preprocess_pipeline_Y.inverse_transform(dummy_df)\n",
    "\n",
    "    # Extract the relevant target column\n",
    "    y_pred = dummy_df_inv[target].to_numpy()\n",
    "\n",
    "    # Evaluate\n",
    "    y_test = Y_test[target]\n",
    "    test_mae = mean_absolute_error(y_test, y_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Absolute Error (MAE) [Testing]: {test_mae:.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE) [Testing]: {test_rmse:.2f}\")\n",
    "    print(f\"Coefficient of Determination (R2) [Testing]: {test_r2:.2f}\")\n",
    "\n",
    "    # Instead of returning a tuple, return a YTargetMetrics object\n",
    "    return YTargetMetrics(target, test_mae, test_rmse, test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate base model\n",
    "# model_xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# # Define search space\n",
    "# search_space = {\n",
    "#     'n_estimators': [100, 200, 400],\n",
    "#     'max_depth': [3, 6, 9],\n",
    "#     'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "#     'subsample': [0.5, 0.8, 1.0],\n",
    "#     'gamma': [0.01, 0.1]\n",
    "# }\n",
    "# # set output folder\n",
    "# output_loc = '../nb/out_files'\n",
    "# # initialize an empty dict\n",
    "# tuned_results = {}\n",
    "\n",
    "# for target in Y_labels_all:\n",
    "#     best_model, best_params, best_metric = run_grid_search_cv(model_xgb, target, search_space, X_train_yjs, Y_train_yjs, output_loc)\n",
    "    \n",
    "#     tuned_metrics = predict_with_best_xgboost_model(best_model, target, X_test_yjs, Y_test_yjs, Y_test, preprocess_pipeline_Y)\n",
    "\n",
    "#     # Store the metric result in the dictionary\n",
    "#     tuned_results[target] = tuned_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming tuned_results is your dictionary of YTargetMetrics instances\n",
    "\n",
    "# # Extract R2 scores into a DataFrame\n",
    "# r2_data = {target: metrics.test_r2 for target, metrics in tuned_results.items()}\n",
    "# df_r2 = pd.DataFrame(list(r2_data.items()), columns=['Target', 'R2'])\n",
    "\n",
    "# # Create a heatmap\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# heatmap_data = df_r2.set_index('Target')\n",
    "# sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.4f')\n",
    "# plt.title('R2 Scores Heatmap for Different Targets')\n",
    "# plt.show()\n",
    "\n",
    "# # Create a bar plot\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x='Target', y='R2', data=df_r2)\n",
    "# plt.title('R2 Scores Comparison for Different Targets')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
