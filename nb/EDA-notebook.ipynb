{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in latest data\n",
    "# the ZL updated metadata with IHC already excluded\n",
    "df = pd.read_csv(\"../input-data/SA/data_updated230524_new_excludedIHC.tsv\",sep=\"\\t\")\n",
    "print(df.shape)\n",
    "\n",
    "#print row 107 and 226\n",
    "df.iloc[[107,226], 1:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the 29 Cibersort scores, leaving only 3\n",
    "dfd = df.drop(columns=['Bindea_full', 'Expanded_IFNg', \n",
    "        'C_Bcellsmemory','C_Plasmacells','C_TcellsCD8','C_TcellsCD4naive',\n",
    "         'C_TcellsCD4memoryactivated','C_Tcellsfollicularhelper',\n",
    "         'C_Tcellsregulatory(Tregs)','C_Tcellsgammadelta','C_NKcellsresting',\n",
    "         'C_NKcellsactivated', 'C_Monocytes', 'C_MacrophagesM0',\n",
    "         'C_MacrophagesM1','C_Dendriticcellsresting',\n",
    "         'C_Dendriticcellsactivated', 'C_Mastcellsresting',\n",
    "         'C_Mastcellsactivated','C_Eosinophils', 'C_Neutrophils', 'S_PAM100HRD'])\n",
    "\n",
    "print(dfd.shape)\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset df into just TotalNeo_Count (as X variables) and the immune scores as Y variables\n",
    "dfd_x = dfd.drop(columns = ['PAM50', 'Subtype', 'HR_status',\t'HER_status', 'Age', 'AgeGroup', 'Stage', 'TumorGrade', 'TumourSize', 'FusionNeo_Count', 'FusionNeo_bestScore','FusionTransscript_Count', 'Fusion_T2NeoRate', 'SNVindelNeo_Count', 'SNVindelNeo_IC50', 'SNVindelNeo_IC50Percentile'])\n",
    "\n",
    "print(dfd_x.shape)\n",
    "dfd_x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts do not tally so there has to be NaNs\n",
    "dfd_x.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop all NaN for now\n",
    "dfd_xc = dfd_x.dropna()\n",
    "\n",
    "print(dfd_xc.shape)\n",
    "print(dfd_xc.isnull().sum())\n",
    "dfd_xc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ID column as index and subset the rest of the columns (from col 1 to 5)\n",
    "dfd_ss = dfd_xc.set_index('ID')\n",
    "ss_cols = list(dfd_ss.columns)\n",
    "print(ss_cols)\n",
    "print(len(ss_cols))\n",
    "dfd_ss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's programmatically subset of the huge dataframe into subsets of columns so we can plot a scatter plot matrix manageably. Define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "def subset_df_by_columns(df: pd.DataFrame, num_subsets: int, x_variable: str) -> typing.DefaultDict:\n",
    "    \"\"\"Subset a DataFrame into approximately equal groups of columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        num_subsets (int): The desired number of subsets.\n",
    "        x_variable (str): The name of the column to use as the x-axis variable.\n",
    "\n",
    "    Returns:\n",
    "        typing.DefaultDict: A dictionary containing the subsets, where the keys are the subset indices\n",
    "    \"\"\"\n",
    "    if df.columns[0] != 'Batch' and df.columns[1] != x_variable:\n",
    "        raise ValueError(f\"The first two columns of the DataFrame must be 'Batch' and the specified X variable ({x_variable}).\")\n",
    "\n",
    "    # slice the dataframe\n",
    "    df_x = df.iloc[:, :2]\n",
    "    df_y = df.iloc[:, 2:]\n",
    "    # Get the number of columns in the remaining DataFrame\n",
    "    num_col_Y = len(df_y.columns)\n",
    "\n",
    "    # Calculate the number of columns per subset in the Y var df\n",
    "    cols_per_subset, remainder = divmod(num_col_Y, num_subsets)\n",
    "\n",
    "    # Create a list of column indices for each subset\n",
    "    col_indices = []\n",
    "    start = 0\n",
    "    for i in range(num_subsets):\n",
    "        end = start + cols_per_subset\n",
    "        if i < remainder:\n",
    "            end += 1\n",
    "        col_indices.append(list(range(start, end)))\n",
    "        start = end\n",
    "\n",
    "    # Subset the DataFrame based on the column indices\n",
    "    subsets = {i: df_y.iloc[:, indices] for i, indices in enumerate(col_indices)}\n",
    "\n",
    "    # map a concat operation for all the dfs in the dict\n",
    "    for key, value in subsets.items():\n",
    "        subsets[key] = pd.concat([df_x, value], axis=1)\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ss = 12\n",
    "print(len(ss_dict := subset_df_by_columns(dfd_ss, num_ss, 'TotalNeo_Count')))\n",
    "ss_dict[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X variable TotalNeo_Count should be transformed due to massive outliers\n",
    "# Apply log transformation as a map to the dictionary of dfs\n",
    "\n",
    "##### DEPRECATED ######\n",
    "# ss_logtrans_dict = {\n",
    "#     # apply log transform just on the TotalNeo_Count\n",
    "#     # key: df.assign(TotalNeo_Count=lambda x: np.log1p(x['TotalNeo_Count']))\n",
    "#     # log transform all instead of just Total Neo Count\n",
    "#     key: df[['Batch']].join(df.drop('Batch', axis=1).apply(np.log1p))\n",
    "#     for key, df in ss_dict.items()\n",
    "# }\n",
    "\n",
    "# IMPRES column is a discrete score so it does not make sense to have it log-transformed. Redo\n",
    "\n",
    "ss_logtrans_dict = {}\n",
    "\n",
    "for key, df in ss_dict.items():\n",
    "    if key == 0:\n",
    "        ss_logtrans_dict[key] = df[['Batch', 'IMPRES']].join(df.drop(['Batch', 'IMPRES'], axis=1).apply(np.log1p))\n",
    "        # switch the position of IMPRES with TotalNeo_Count columns with each other\n",
    "        col_tokeep = [col for col in ss_logtrans_dict[key].columns if col not in ['Batch', 'TotalNeo_Count', 'IMPRES']]\n",
    "        new_order = ['Batch', 'TotalNeo_Count', 'IMPRES'] + col_tokeep\n",
    "        ss_logtrans_dict[key] = ss_logtrans_dict[key][new_order]\n",
    "    else:\n",
    "        ss_logtrans_dict[key] = df[['Batch']].join(df.drop('Batch', axis=1).apply(np.log1p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_logtrans_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot untransformed subset \n",
    "# test a subset df\n",
    "set_num = 0\n",
    "pp = sns.pairplot(ss_dict[set_num], hue='Batch', diag_kind=\"kde\", kind='reg', corner=True, plot_kws={'scatter_kws': {'alpha': 0.5, 's': 10}}, palette='Set1')\n",
    "# add plot title\n",
    "plt.suptitle(f'Raw Total Neoantigen Count vs Immune Features (Set No. {set_num})', fontsize=28, fontweight='medium')\n",
    "\n",
    "# Iterate through the axes and set bold titles\n",
    "for i, ax in enumerate(pp.axes.flat):\n",
    "    if ax is not None:\n",
    "        if ax.get_xlabel() == \"TotalNeo_Count\":\n",
    "            ax.set_xlabel(ax.get_xlabel(), fontweight='bold', fontsize=12, color='red')\n",
    "        else:\n",
    "            ax.set_xlabel(ax.get_xlabel(), fontweight='bold', fontsize=12)\n",
    "        \n",
    "        # Handle y-axis labels (only for the leftmost column)\n",
    "        if i % pp.axes.shape[1] == 0:  # Check if it's the first column\n",
    "            if ax.get_ylabel() == \"TotalNeo_Count\":\n",
    "                ax.set_ylabel(ax.get_ylabel(), fontweight='bold', fontsize=12, color='red')\n",
    "            else:\n",
    "                ax.set_ylabel(ax.get_ylabel(), fontweight='bold', fontsize=12)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replot on the log-transformed data of all columns except IMPRES\n",
    "# test a subset df\n",
    "set_num = 0\n",
    "pp = sns.pairplot(ss_logtrans_dict[0], hue='Batch', diag_kind=\"kde\", kind='reg', corner=True, plot_kws={'scatter_kws': {'alpha': 0.5, 's': 10}}, palette='Set1')\n",
    "# add plot title\n",
    "plt.suptitle(f'Log-Transformed Total Neoantigen Count vs Immune Features (Set No. {set_num})', fontsize=28, fontweight='medium')\n",
    "\n",
    "# Iterate through the axes and set bold titles\n",
    "for i, ax in enumerate(pp.axes.flat):\n",
    "    if ax is not None:\n",
    "        if ax.get_xlabel() == \"TotalNeo_Count\":\n",
    "            ax.set_xlabel(ax.get_xlabel(), fontweight='bold', fontsize=12, color='red')\n",
    "        else:\n",
    "            ax.set_xlabel(ax.get_xlabel(), fontweight='bold', fontsize=12)\n",
    "        \n",
    "        # Handle y-axis labels (only for the leftmost column)\n",
    "        if i % pp.axes.shape[1] == 0:  # Check if it's the first column\n",
    "            if ax.get_ylabel() == \"TotalNeo_Count\":\n",
    "                ax.set_ylabel(ax.get_ylabel(), fontweight='bold', fontsize=12, color='red')\n",
    "            else:\n",
    "                ax.set_ylabel(ax.get_ylabel(), fontweight='bold', fontsize=12)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import gc\n",
    "\n",
    "@contextmanager\n",
    "def plot_and_save(output_path, naming_var):\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        plt.savefig(f'{output_path}/{naming_var}.pdf', dpi=300)\n",
    "        plt.close()\n",
    "        gc.collect()\n",
    "\n",
    "def process_pairplots(df: pd.DataFrame, output_path: str, naming_var: str, hue_col: str = None):\n",
    "    \"\"\"\n",
    "    Generates Seaborn pairplots for the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        output_path (str): The path to save the plots.\n",
    "        naming_var (str): The variable name string to use for naming the output file.\n",
    "        hue_col (str, optional): The column name string to use for stratifying the scatter plot points.\n",
    "\n",
    "    Example:\n",
    "        process_pairplots(df, 'output_path', 'naming_var', 'hue_col')\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with plot_and_save(output_path, naming_var):\n",
    "        sns.pairplot(df, hue=hue_col, diag_kind=\"kde\", kind='reg', corner=True, plot_kws={'scatter_kws': {'alpha': 0.5, 's': 10}}, palette='Set1')\n",
    "        # add plot title\n",
    "        plt.suptitle(f'Log-Transformed Total Neoantigen Count vs Immune Features ({naming_var})', fontsize=20, fontweight=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Distributions and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first rearrange the columns 'IMPRES' with 'ESTIMATE'\n",
    "# Get the list of column names\n",
    "cols_arr = dfd_ss.columns.tolist()\n",
    "\n",
    "# Find the indices of the columns you want to swap\n",
    "indx_A = cols_arr.index('IMPRES')\n",
    "indx_B = cols_arr.index('ESTIMATE')\n",
    "\n",
    "# Swap the positions\n",
    "cols_arr[indx_A], cols_arr[indx_B] = cols_arr[indx_B], cols_arr[indx_A]\n",
    "\n",
    "# Reindex the DataFrame with the new column order\n",
    "dfda_full = dfd_ss[cols_arr]\n",
    "dfda_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise distributions\n",
    "\n",
    "# Determine the number of rows and columns for the subplot grid\n",
    "nrows = 10\n",
    "ncols = 12\n",
    "\n",
    "#define the plotting function\n",
    "def visualise_distribution(df):\n",
    "    # Create a figure and a grid of subplots\n",
    "    # Flatten the axes array for easy iteration\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(30, 22))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot histograms for each column\n",
    "    for i, column in enumerate(df.columns):\n",
    "        sns.histplot(df[column], kde=False, ax=axes[i], color='green')\n",
    "        axes[i].set_title(column)\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylabel('')\n",
    "\n",
    "    # Hide any remaining empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92) \n",
    "    plt.suptitle(f'Original Distribution of the Dataset', fontsize=20, fontweight='bold')\n",
    "    # plt.savefig(f'Distribution_before.png',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#execute the function\n",
    "visualise_distribution(dfda_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yeo-Johnson Transformation\n",
    "Yeo-Johnson transformation is an extension of the Box-Cox transformation, which falls under the power transformation family. YJ algorithm is designed to handle both positive and negative values in the dataset. Similar to Box-Cox, the Yeo-Johnson transformation aims to stabilize variance, make the data more symmetric, and bring it closer to a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YJ transformation\n",
    "# from scipy.stats import yeojohnson\n",
    "# yj = lambda x: yeojohnson(x)[0]\n",
    "# df=df.apply(yj)\n",
    "\n",
    "# zscore = lambda x : stats.zscore(x)\n",
    "# df_input_yjz_b1 = df_input_yj_b1.apply(zscore)\n",
    "# df_input_yjz_b2 = df_input_yj_b2.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import yeojohnson\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# # and numeric_columns is a list of column names you want to transform\n",
    "\n",
    "# def yj_transform(data):\n",
    "#     transformed, lambda_param = yeojohnson(data)\n",
    "#     return pd.Series(transformed, index=data.index, name=data.name)\n",
    "\n",
    "# # Apply the transformation only to specified numeric columns\n",
    "# df[numeric_columns] = df[numeric_columns].apply(yj_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replot heatmap on all_log-transformed data\n",
    "# corr_df_test = ss_logtrans_dict[0].drop(columns='Batch').corr(method='spearman')\n",
    "# corr_df_test = corr_df_test.round(2)\n",
    "\n",
    "# # Create a mask for the upper triangle\n",
    "# mask = np.triu(np.ones_like(corr_df_test, dtype=bool))\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# # Create the correlation matrix and represent it as a heatmap.\n",
    "# hm = sns.heatmap(corr_df_test, annot = True, cmap = 'coolwarm', square = True, linewidths=0.5, mask=mask, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# # Get current labels\n",
    "# ylabels = hm.get_yticklabels()\n",
    "# xlabels = hm.get_xticklabels()\n",
    "\n",
    "# # Hide the first y-axis label and the last x-axis label\n",
    "# ylabels[0].set_visible(False)\n",
    "# xlabels[-1].set_visible(False)\n",
    "\n",
    "# # Rotate and align the tick labels\n",
    "# plt.setp(xlabels, rotation=45, ha='right')\n",
    "\n",
    "# # Change color of specific x-axis label\n",
    "# for label in xlabels:\n",
    "#     if label.get_text() == \"TotalNeo_Count\":\n",
    "#         label.set_color('red')  # Change color to red\n",
    "#         label.set_fontweight('bold')\n",
    "\n",
    "# # Removes all ticks\n",
    "# hm.tick_params(left=False, bottom=False)\n",
    "\n",
    "# hm.set_title('Dataframe #', fontsize=14, x=0.4)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define a heatmap plot function\n",
    "# from contextlib import contextmanager\n",
    "# import gc\n",
    "\n",
    "# @contextmanager\n",
    "# def plot_and_save(output_path, naming_var):\n",
    "#     try:\n",
    "#         yield\n",
    "#     finally:\n",
    "#         plt.savefig(f'{output_path}/{naming_var}.pdf', dpi=300)\n",
    "#         plt.close()\n",
    "#         gc.collect()\n",
    "\n",
    "# def process_heatmaps(df: pd.DataFrame, output_path: str, naming_var: str):\n",
    "#     with plot_and_save(output_path, naming_var):\n",
    "#         corr_df_test = df.drop(columns='Batch').corr(method='spearman')\n",
    "\n",
    "#         # Create a mask for the upper triangle\n",
    "#         mask = np.triu(np.ones_like(corr_df_test, dtype=bool))\n",
    "\n",
    "#         # Create the correlation matrix and represent it as a heatmap.\n",
    "#         hm = sns.heatmap(corr_df_test, annot = True, cmap = 'coolwarm', square = True, linewidths=0.5, mask=mask, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "#         # Get current labels\n",
    "#         ylabels = hm.get_yticklabels()\n",
    "#         xlabels = hm.get_xticklabels()\n",
    "\n",
    "#         # Hide the first y-axis label and the last x-axis label\n",
    "#         ylabels[0].set_visible(False)\n",
    "#         xlabels[-1].set_visible(False)\n",
    "\n",
    "#         # Removes all ticks\n",
    "#         hm.tick_params(left=False, bottom=False)\n",
    "\n",
    "#         hm.set_title(f'{naming_var}', fontsize=14, x=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Batch Effect Assesment\n",
    "\n",
    "Run PCA to test for potential batch effects in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# # Assuming 'Batch' is your batch column and the rest are features\n",
    "# features = [col for col in dfd_ss.columns if col != 'Batch']\n",
    "# X = dfd_ss[features]\n",
    "# batch = dfd_ss['Batch']\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Perform PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(X_scaled)\n",
    "\n",
    "# # Create a DataFrame with PCA results\n",
    "# pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "# pca_df['Batch'] = batch.values\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Batch', palette='deep')\n",
    "# plt.title('PCA of Dataset Colored by Batch')\n",
    "# plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} explained variance)')\n",
    "# plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} explained variance)')\n",
    "\n",
    "# # Add a legend title\n",
    "# plt.legend(title='Batch')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
