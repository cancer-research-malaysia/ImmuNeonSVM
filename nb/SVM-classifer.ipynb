{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "title: \"Support Vector Machine Classifier\"\n",
    "author: Suffian Azizan\n",
    "date: \"{{ datetime.now().strftime('%Y-%m-%d') }}\"\n",
    "output:\n",
    "    html:\n",
    "        theme: united\n",
    "        code_tools: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine for Classification Problem\n",
    "### *Exploring the association between neoantigen-related variables and immune scores*\n",
    "This notebook is the continuation of the `support_vector_reg.ipynb` notebook, detailing the testing of SVM application on our neoantigen dataset, converted into a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Package and Raw Data Loading**\n",
    "First, import necessary packages and load in the raw data table into `pandas` dataFrame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itables import show\n",
    "from IPython.display import HTML, display\n",
    "from warnings import simplefilter, filterwarnings\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "filterwarnings(\"ignore\", category=UserWarning)\n",
    "pd.set_option('display.max_columns', None)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# load pretty jupyter's magics\n",
    "%load_ext pretty_jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the cleaned-up dataset wrangled from MH's latest work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in latest data\n",
    "# use the 202409_new_excludedIHC_batch-duplicate-removed.tsv\n",
    "df = pd.read_csv(\"../input-data/SA/202409_new_excludedIHC_batch-duplicate-removed.tsv\",sep=\"\\t\")\n",
    "print(f\"Before trimming columns: {df.shape}\")\n",
    "\n",
    "# exclude the 29 Cibersort scores, leaving only 3\n",
    "df = df.drop(columns=['Bindea_full', 'Expanded_IFNg', \n",
    "        'C_Bcellsmemory','C_Plasmacells','C_TcellsCD8','C_TcellsCD4naive',\n",
    "         'C_TcellsCD4memoryactivated','C_Tcellsfollicularhelper',\n",
    "         'C_Tcellsregulatory(Tregs)','C_Tcellsgammadelta','C_NKcellsresting',\n",
    "         'C_NKcellsactivated', 'C_Monocytes', 'C_MacrophagesM0',\n",
    "         'C_MacrophagesM1','C_Dendriticcellsresting',\n",
    "         'C_Dendriticcellsactivated', 'C_Mastcellsresting',\n",
    "         'C_Mastcellsactivated','C_Eosinophils', 'C_Neutrophils', 'S_PAM100HRD'])\n",
    "\n",
    "print(f\"After trimming columns: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Preprocessing**\n",
    "\n",
    "Decide all the clinical variables and neoantigen-related variables to keep in the X matrix (features).\n",
    "\n",
    "1. `Subtype` column has already been encoded categorically by `HR_status` and `HER_status` columns so these two columns can be dropped. ***UPDATE: due to their lesser importance during the default XGBoost modeling, `PAM50` column was dropped as well.***\n",
    "\n",
    "2.  `AgeGroup` is just a binned information of `Age` column so it is dropped as it is redundant.\n",
    "\n",
    "~~3. Drop `FusionNeo_bestScore`, `FusionTransscript_Count`, `Fusion_T2NeoRate` columns as well as the `SNVindelNeo_IC50` and `SNVindelNeo_IC50Percentile` columns for now to reduce complexity.~~\n",
    "\n",
    "4. Drop `Batch` column.\n",
    "\n",
    "~~> **UPDATE 1: Exclude `TotalNeo_Count`, and include `Fusion_T2NeoRate` and `SNVindelNeo_IC50` columns.~~ Also, rename `Fusion_T2NeoRate` to `FN/FT_Ratio`.**\n",
    "\n",
    "> **UPDATE 2: put back `FusionNeo_bestScore` into the X variable set and rename it into `FusionNeo_bestIC50`**\n",
    "\n",
    "NOTE: The FN/FT_Ratio can go beyond 1. In this case this implies that there are more predicted neoantigens than there are predicted transcript. Imagine one transcript being able to produce more than 1 putative neoantigen. This implies a highly immunogenic transcript, so this metric might be useful down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop all NaN for now and set col 'ID' as index\n",
    "# dfd = df.drop(columns = ['Batch', 'Stage', 'PAM50', 'HR_status', 'HER_status', 'AgeGroup', 'TotalNeo_Count', 'FusionTransscript_Count', 'SNVindelNeo_IC50Percentile']).dropna().set_index('ID')\n",
    "dfd = df.drop(columns = ['Batch', 'Stage', 'PAM50', 'HR_status', 'HER_status', 'AgeGroup', 'SNVindelNeo_IC50Percentile']).dropna().set_index('ID')\n",
    "\n",
    "# rename the column `Fusion_T2NeoRate` to `FN/FT_Ratio` and `FusionNeo_bestScore` to `FusionNeo_bestIC50`\n",
    "dfd.rename(columns={'Fusion_T2NeoRate': 'FN/FT_Ratio'}, inplace=True)\n",
    "dfd.rename(columns={'FusionNeo_bestScore': 'FusionNeo_bestIC50'}, inplace=True)\n",
    "dfd.rename(columns={'FusionTransscript_Count': 'FusionTranscript_Count'}, inplace=True)\n",
    "\n",
    "print(dfd.shape)\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** Check to make sure there is no duplicated index rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfd.index[dfd.index.duplicated()].unique())\n",
    "rows_dupe = list(dfd.index[dfd.index.duplicated()].unique())\n",
    "rows_dupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We need to encode the `object` columns of `Subtype` and `FN/FT_Ratio` into appropriate types. Change `Age`, `TumorGrade`, and `IMPRES` into `int64` as well as all `*_Count` columns because they are discrete variables. Change the `FN/FT_Ratio` into `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd['Subtype'] = dfd['Subtype'].astype('category')\n",
    "dfd['Age'] = dfd['Age'].astype('int64')\n",
    "dfd['TumorGrade'] = dfd['TumorGrade'].astype('int64')\n",
    "dfd['IMPRES'] = dfd['IMPRES'].astype('int64')\n",
    "dfd['FusionNeo_Count'] = dfd['FusionNeo_Count'].astype('int64')\n",
    "dfd['FusionTranscript_Count'] = dfd['FusionTranscript_Count'].astype('int64')\n",
    "dfd['SNVindelNeo_Count'] = dfd['SNVindelNeo_Count'].astype('int64')\n",
    "dfd['FN/FT_Ratio'] = dfd['FN/FT_Ratio'].astype('float64')\n",
    "\n",
    "# print(dfd.dtypes)\n",
    "pd.set_option('display.max_rows', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dfd dataframe to a new file as a pandas object\n",
    "dfd.to_pickle(\"../input-data/SA/202409_new_excludedIHC_batch-duplicate-removed_cols-dropped_coltypes_encoded.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Feature_Engine's `OneHotEncoder()` to create a `k` dummy variable set for `Subtype`.\n",
    "\n",
    "**NOTE**: The encoded columns will be appended at the end of the dataFrame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    variables=['Subtype'],\n",
    "    drop_last=False)\n",
    "\n",
    "encoder.fit(dfd)\n",
    "dfd_ = encoder.transform(dfd)\n",
    "dfd_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the encoded columns to shift\n",
    "enc_cols = ['Subtype_HR+/HER2-', 'Subtype_HR+/HER2+', 'Subtype_TNBC', 'Subtype_HR-/HER2+']\n",
    "\n",
    "# Drop the specified columns and store them\n",
    "encoded_df = dfd_[enc_cols]\n",
    "dfenc = dfd.drop(columns=['Subtype'])\n",
    "\n",
    "# Specify the index where you want to reinsert the columns\n",
    "insert_index = 0  # This will insert at the first column\n",
    "\n",
    "# Reinsert the columns\n",
    "for i, col in enumerate(encoded_df.columns):\n",
    "    dfenc.insert(insert_index + i, col, encoded_df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the categorically-encoded dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfenc.shape)\n",
    "dfenc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below is the original, unencoded dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfd.shape)\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~#### **Subsetting Y Labels**~~\n",
    "\n",
    "~~In the previous exploration, many of the immune scores (Y targets/labels) might not really show much relationship with fusion neoantigen variables so they may not be as informative. We decided to use Caitlin's finding and subset the Y labels into several clinically meaningful groups.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use the unencoded categorical dataframe (dfd) and drop the Subtype categorical column\n",
    "# df_dcat = dfd.drop(columns=['Subtype'])\n",
    "# print(df_dcat.shape)\n",
    "# df_dcat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First list all the clinical variables that would be the X feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = ['Subtype_HR+/HER2-', 'Subtype_HR+/HER2+', 'Subtype_TNBC', 'Subtype_HR-/HER2+', 'Age', 'TumorGrade', 'TumourSize', 'FusionNeo_Count', 'FusionNeo_bestIC50', 'FusionTranscript_Count', 'FN/FT_Ratio', 'SNVindelNeo_Count', 'SNVindelNeo_IC50', 'TotalNeo_Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_nocat = ['Age', 'TumorGrade', 'TumourSize', 'FusionNeo_Count', 'FusionNeo_bestIC50', 'FusionTranscript_Count', 'FN/FT_Ratio', 'SNVindelNeo_Count', 'SNVindelNeo_IC50', 'TotalNeo_Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the Y variable set\n",
    "Y_labels_all = [col for col in dfd.drop(columns=['Subtype']).columns if col not in X_features]\n",
    "print(Y_labels_all[:5])\n",
    "len(Y_labels_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Split Dataset with `train_test_split` & Create a Data Transformation Pipeline from `feature_engine` Package**\n",
    "\n",
    "Split the dataset before modeling to avoid information leakage.\n",
    "\n",
    "Next, the pipeline will apply the Yeo-Johnson transformation on the split datasets on select X features and all Y labels, and scale them using `StandardScaler` (but wrapped within `feature_engine`'s wrapper) on select X features and all Y labels.\n",
    "\n",
    "This pipeline would enable easy inverse transform steps for both X and Y datasets later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset X columns as desired\n",
    "X = dfenc[X_features]\n",
    "X = X.drop(columns=[\"FusionNeo_bestIC50\", \"SNVindelNeo_IC50\"])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of the X columns\n",
    "X.hist(figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grab the Y targets (do this as a whole, but we will train on each column individually later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the Y variable set\n",
    "Y = dfenc[Y_labels_all]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.hist(figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform train test split on the X and Y variables then create a data transformation pipeline to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables to scale\n",
    "cols_X = X_train.columns.tolist()\n",
    "scale_cols_X = [col for col in cols_X if col not in ['Subtype_HR+/HER2-', 'Subtype_HR+/HER2+', 'Subtype_TNBC', 'Subtype_HR-/HER2+']]\n",
    "scale_cols_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_X = ['Subtype_HR+/HER2-', 'Subtype_HR+/HER2+', 'Subtype_TNBC', 'Subtype_HR-/HER2+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables to target\n",
    "cols_Y = Y_train.columns.tolist()\n",
    "# drop ESTIMATE and IMPRES\n",
    "target_cols_Y = [col for col in cols_Y if col not in ['ESTIMATE', 'IMPRES']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Binning**\n",
    "\n",
    "To test SVM, we need to bin the target Y datasets into discrete classes. We can leave X datasets as they are (transformed and scaled). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.pipeline import Pipeline\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "\n",
    "# Separate numeric and categorical preprocessing\n",
    "numeric_transformer = Pipeline([\n",
    "    ('yeo_johnson', YeoJohnsonTransformer(variables=scale_cols_X)),\n",
    "    ('scaler', SklearnTransformerWrapper(transformer=StandardScaler(), variables=scale_cols_X))\n",
    "])\n",
    "\n",
    "# Create column transformer for X preprocessing\n",
    "preprocess_pipeline_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', numeric_transformer, scale_cols_X),\n",
    "        ('categorical', 'passthrough', categorical_cols_X)  # Assuming categorical_cols is your list of one-hot columns\n",
    "    ],\n",
    "    remainder='passthrough'  # This will pass through any columns not explicitly specified\n",
    ")\n",
    "\n",
    "# Simplified Y preprocessing - only binning for classification\n",
    "class MultiColumnBinner:\n",
    "    def __init__(self, n_bins=3, strategy='quantile'):\n",
    "        self.n_bins = n_bins\n",
    "        self.strategy = strategy\n",
    "        self.binner = None\n",
    "        self.bin_edges_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.binner = KBinsDiscretizer(\n",
    "            n_bins=self.n_bins,\n",
    "            encode='ordinal',\n",
    "            strategy=self.strategy\n",
    "        )\n",
    "        self.binner.fit(X)\n",
    "        self.bin_edges_ = self.binner.bin_edges_\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        binned = self.binner.transform(X)\n",
    "        return pd.DataFrame(\n",
    "            binned,\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "# Simplified Y pipeline - only binning\n",
    "preprocess_pipeline_Y = Pipeline([\n",
    "    ('binner', MultiColumnBinner(n_bins=3, strategy='quantile'))\n",
    "])\n",
    "\n",
    "# Modified process X data - note that we need to convert to DataFrame after transformation\n",
    "preprocess_pipeline_X.fit(X_train)\n",
    "X_train_transformed = pd.DataFrame(\n",
    "    preprocess_pipeline_X.transform(X_train),\n",
    "    columns=list(scale_cols_X) + list(categorical_cols_X),  # Maintain column names\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_transformed = pd.DataFrame(\n",
    "    preprocess_pipeline_X.transform(X_test),\n",
    "    columns=list(scale_cols_X) + list(categorical_cols_X),  # Maintain column names\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Process Y data - now only binning\n",
    "preprocess_pipeline_Y.fit(Y_train)\n",
    "Y_train_binned = preprocess_pipeline_Y.transform(Y_train)\n",
    "Y_test_binned = preprocess_pipeline_Y.transform(Y_test)\n",
    "\n",
    "# Print bin edges for interpretation\n",
    "binner = preprocess_pipeline_Y.steps[0][1]\n",
    "for col, edges in zip(Y_train.columns, binner.bin_edges_):\n",
    "    print(f\"\\nBin edges for {col}:\")\n",
    "    print(f\"low (0): <= {edges[1]:.2f}\")\n",
    "    print(f\"mid (1): {edges[1]:.2f} to {edges[2]:.2f}\")\n",
    "    print(f\"high (2): > {edges[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_model(\n",
    "    y_target: str,\n",
    "    Y_train_binned: pd.DataFrame,\n",
    "    Y_test_binned: pd.DataFrame,\n",
    "    X_train_transformed: pd.DataFrame,\n",
    "    X_test_transformed: pd.DataFrame,\n",
    ") -> Dict:\n",
    "    \"\"\"Run classification model for a single target variable and return performance metrics.\"\"\"\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    \n",
    "    # Get binned target data\n",
    "    y_train = Y_train_binned[y_target].astype(int)\n",
    "    y_test = Y_test_binned[y_target].astype(int)\n",
    "    \n",
    "    # Initialize and fit model\n",
    "    model = SVC(kernel='rbf', probability=True)\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train_transformed)\n",
    "    y_test_pred = model.predict(X_test_transformed)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Generate classification report\n",
    "    test_report = classification_report(y_test, y_test_pred)\n",
    "    \n",
    "    # Create and save confusion matrix plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_train, y_train_pred,\n",
    "        display_labels=['low', 'mid', 'high'],\n",
    "        ax=ax1\n",
    "    )\n",
    "    ax1.set_title('Training Confusion Matrix')\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred,\n",
    "        display_labels=['low', 'mid', 'high'],\n",
    "        ax=ax2\n",
    "    )\n",
    "    ax2.set_title('Testing Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    plt.savefig(f'plots-classification/{y_target}/{y_target}-classification-confusion-matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'target_name': y_target,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'classification_report': test_report\n",
    "    }\n",
    "\n",
    "# Example usage for all targets\n",
    "results_dict = {}\n",
    "for y_target in Y_train.columns:\n",
    "    print(f\"\\nProcessing target: {y_target}\")\n",
    "    results = run_classification_model(\n",
    "        y_target=y_target,\n",
    "        Y_train_binned=Y_train_binned,\n",
    "        Y_test_binned=Y_test_binned,\n",
    "        X_train_transformed=X_train_transformed,\n",
    "        X_test_transformed=X_test_transformed\n",
    "    )\n",
    "    results_dict[y_target] = results\n",
    "    \n",
    "    print(f\"Results for {y_target}:\")\n",
    "    print(f\"Train accuracy: {results['train_accuracy']:.4f}\")\n",
    "    print(f\"Test accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def analyze_model_performances(results_dict: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze performance metrics across all target variables.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary containing results from run_classification_model\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sorted performance metrics\n",
    "    \"\"\"\n",
    "    # Create a list to store performance metrics\n",
    "    performances = []\n",
    "    \n",
    "    for target, results in results_dict.items():\n",
    "        performances.append({\n",
    "            'target': target,\n",
    "            'train_accuracy': results['train_accuracy'],\n",
    "            'test_accuracy': results['test_accuracy'],\n",
    "            'difference': results['train_accuracy'] - results['test_accuracy']  # to check overfitting\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    perf_df = pd.DataFrame(performances)\n",
    "    \n",
    "    # Sort by test accuracy (primary metric) in descending order\n",
    "    perf_df_sorted = perf_df.sort_values('test_accuracy', ascending=False)\n",
    "    \n",
    "    # Print the best performing target\n",
    "    best_target = perf_df_sorted.iloc[0]\n",
    "    print(\"\\nBest performing target:\")\n",
    "    print(f\"Target: {best_target['target']}\")\n",
    "    print(f\"Test Accuracy: {best_target['test_accuracy']:.4f}\")\n",
    "    print(f\"Train Accuracy: {best_target['train_accuracy']:.4f}\")\n",
    "    print(f\"Difference (Train-Test): {best_target['difference']:.4f}\")\n",
    "    \n",
    "    print(\"\\nAll targets sorted by test accuracy:\")\n",
    "    print(perf_df_sorted.to_string(float_format=lambda x: '{:.4f}'.format(x)))\n",
    "    \n",
    "    # Identify potential overfitting cases\n",
    "    overfitting_threshold = 0.1  # You can adjust this threshold\n",
    "    overfitting_cases = perf_df[perf_df['difference'] > overfitting_threshold]\n",
    "    \n",
    "    if not overfitting_cases.empty:\n",
    "        print(\"\\nPotential overfitting cases (Train-Test > 0.1):\")\n",
    "        print(overfitting_cases.to_string(float_format=lambda x: '{:.4f}'.format(x)))\n",
    "    \n",
    "    return perf_df_sorted\n",
    "\n",
    "# After running your models, use this to analyze:\n",
    "performance_summary = analyze_model_performances(results_dict)\n",
    "\n",
    "# If you want to get just the top N performing targets\n",
    "N = 5  # Change this to get more or fewer top performers\n",
    "top_N_targets = performance_summary.head(N)\n",
    "print(f\"\\nTop {N} performing targets:\")\n",
    "print(top_N_targets.to_string(float_format=lambda x: '{:.4f}'.format(x)))\n",
    "\n",
    "# Get the classification report for the best performing target\n",
    "best_target = performance_summary.iloc[0]['target']\n",
    "print(f\"\\nDetailed classification report for best target ({best_target}):\")\n",
    "print(results_dict[best_target]['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Iterative Learning over Y Labels with `GridSearchCV` for Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rewrite the functions to incorporate `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# def run_svr_model_gridsearch(\n",
    "#     y_target: str,\n",
    "#     Y_train: pd.DataFrame,\n",
    "#     Y_test: pd.DataFrame,\n",
    "#     X_train_transformed: pd.DataFrame,\n",
    "#     X_test_transformed: pd.DataFrame,\n",
    "#     Y_train_transformed: pd.DataFrame,\n",
    "#     Y_test_transformed: pd.DataFrame,\n",
    "#     preprocess_pipeline_Y: Pipeline\n",
    "# ) -> YTargetMetrics:\n",
    "#     \"\"\"Run SVR model with GridSearchCV for a single target variable and return performance metrics.\"\"\"\n",
    "\n",
    "#     # Define parameter grid\n",
    "#     param_grid = {\n",
    "#         'C': [0.1, 1, 10, 100],\n",
    "#         'epsilon': [0.01, 0.1, 0.2],\n",
    "#         'gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "#     }\n",
    "\n",
    "#     # Initialize base model\n",
    "#     base_model = SVR()\n",
    "\n",
    "#     # Setup GridSearchCV\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=base_model,\n",
    "#         param_grid=param_grid,\n",
    "#         cv=5,\n",
    "#         scoring='neg_mean_squared_error',\n",
    "#         n_jobs=-1,\n",
    "#         verbose=1\n",
    "#     )\n",
    "\n",
    "#     # Fit GridSearchCV\n",
    "#     print(f\"\\nPerforming GridSearchCV for {y_target}...\")\n",
    "#     grid_search.fit(X_train_transformed, Y_train_transformed[y_target])\n",
    "\n",
    "#     # Print best parameters\n",
    "#     print(f\"\\nBest parameters for {y_target}:\")\n",
    "#     print(grid_search.best_params_)\n",
    "\n",
    "#     # Use best model for predictions\n",
    "#     model_instance = grid_search.best_estimator_\n",
    "    \n",
    "#     # Get predictions (transformed space)\n",
    "#     y_train_pred_transformed = model_instance.predict(X_train_transformed)\n",
    "#     y_test_pred_transformed = model_instance.predict(X_test_transformed)\n",
    "\n",
    "#     # Create dummy DataFrames for inverse transform\n",
    "#     dummy_train_y = pd.DataFrame(0, index=X_train_transformed.index, \n",
    "#                                 columns=Y_train_transformed.columns)\n",
    "#     dummy_train_y[y_target] = y_train_pred_transformed\n",
    "\n",
    "#     dummy_test_y = pd.DataFrame(0, index=X_test_transformed.index, \n",
    "#                                columns=Y_test_transformed.columns)\n",
    "#     dummy_test_y[y_target] = y_test_pred_transformed\n",
    "\n",
    "#     # Inverse transform predictions\n",
    "#     dummy_train_y_inv = preprocess_pipeline_Y.inverse_transform(dummy_train_y)\n",
    "#     dummy_test_y_inv = preprocess_pipeline_Y.inverse_transform(dummy_test_y)\n",
    "\n",
    "#     # Extract the relevant target column\n",
    "#     y_train_pred = dummy_train_y_inv[y_target].to_numpy()\n",
    "#     y_test_pred = dummy_test_y_inv[y_target].to_numpy()\n",
    "\n",
    "#     # Get raw target data\n",
    "#     raw_y_train = Y_train[y_target]\n",
    "#     raw_y_test = Y_test[y_target]\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     train_r2 = r2_score(raw_y_train, y_train_pred)\n",
    "#     test_r2 = r2_score(raw_y_test, y_test_pred)\n",
    "#     train_rmse = np.sqrt(mean_squared_error(raw_y_train, y_train_pred))\n",
    "#     test_rmse = np.sqrt(mean_squared_error(raw_y_test, y_test_pred))\n",
    "#     train_mae = mean_absolute_error(raw_y_train, y_train_pred)\n",
    "#     test_mae = mean_absolute_error(raw_y_test, y_test_pred)\n",
    "\n",
    "#     # Create plots directory\n",
    "#     os.makedirs(f'plots/{y_target}', exist_ok=True)\n",
    "\n",
    "#     # Plot CV results\n",
    "#     cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(cv_results['param_C'], -cv_results['mean_test_score'], 'o-')\n",
    "#     plt.xlabel('C parameter')\n",
    "#     plt.ylabel('Mean Squared Error')\n",
    "#     plt.xscale('log')\n",
    "    \n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(cv_results['param_epsilon'], -cv_results['mean_test_score'], 'o-')\n",
    "#     plt.xlabel('Epsilon parameter')\n",
    "#     plt.ylabel('Mean Squared Error')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'plots/{y_target}/{y_target}-SVR-grid-search-results.png')\n",
    "#     plt.close()\n",
    "\n",
    "#     # Plot actual vs predicted\n",
    "#     _, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), dpi=300)\n",
    "\n",
    "#     ax1.scatter(raw_y_train, y_train_pred, alpha=0.5)\n",
    "#     ax1.plot([raw_y_train.min(), raw_y_train.max()], \n",
    "#              [raw_y_train.min(), raw_y_train.max()], 'r--', lw=2)\n",
    "#     ax1.set_xlabel('Actual')\n",
    "#     ax1.set_ylabel('Predicted')\n",
    "#     ax1.set_title('Training Set')\n",
    "#     ax1.grid(True)\n",
    "\n",
    "#     ax2.scatter(raw_y_test, y_test_pred, alpha=0.5)\n",
    "#     ax2.plot([raw_y_test.min(), raw_y_test.max()], \n",
    "#              [raw_y_test.min(), raw_y_test.max()], 'r--', lw=2)\n",
    "#     ax2.set_xlabel('Actual')\n",
    "#     ax2.set_ylabel('Predicted')\n",
    "#     ax2.set_title('Testing Set')\n",
    "#     ax2.grid(True)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'plots/{y_target}/{y_target}-SVR-tuned-model-performance-comparison.png')\n",
    "#     plt.close()\n",
    "\n",
    "#     # Create results object with additional grid search info\n",
    "#     results = YTargetMetrics(y_target, train_r2, test_r2, train_rmse, test_rmse, train_mae, test_mae)\n",
    "    \n",
    "#     # Add grid search results to dictionary\n",
    "#     grid_search_results = {\n",
    "#         'best_params': grid_search.best_params_,\n",
    "#         'best_score': -grid_search.best_score_,  # Convert back from negative MSE\n",
    "#         'cv_results': grid_search.cv_results_\n",
    "#     }\n",
    "\n",
    "#     return results, grid_search_results\n",
    "\n",
    "# # Modified function to handle multiple targets\n",
    "# def run_svr_for_multiple_targets(\n",
    "#     y_columns: list[str],\n",
    "#     Y_train: pd.DataFrame,\n",
    "#     Y_test: pd.DataFrame,\n",
    "#     X_train_transformed: pd.DataFrame,\n",
    "#     X_test_transformed: pd.DataFrame,\n",
    "#     Y_train_transformed: pd.DataFrame,\n",
    "#     Y_test_transformed: pd.DataFrame,\n",
    "#     preprocess_pipeline_Y: Pipeline\n",
    "# ) -> Dict[str, tuple[YTargetMetrics, dict]]:\n",
    "    \n",
    "#     results_dict = {}\n",
    "    \n",
    "#     for y_target in y_columns:\n",
    "#         try:\n",
    "#             print(f\"\\nProcessing target: {y_target}\")\n",
    "            \n",
    "#             metrics, grid_results = run_svr_model_gridsearch(\n",
    "#                 y_target=y_target,\n",
    "#                 Y_train=Y_train,\n",
    "#                 Y_test=Y_test,\n",
    "#                 X_train_transformed=X_train_transformed,\n",
    "#                 X_test_transformed=X_test_transformed,\n",
    "#                 Y_train_transformed=Y_train_transformed,\n",
    "#                 Y_test_transformed=Y_test_transformed,\n",
    "#                 preprocess_pipeline_Y=preprocess_pipeline_Y\n",
    "#             )\n",
    "            \n",
    "#             results_dict[y_target] = (metrics, grid_results)\n",
    "#             print(f\"\\nResults for {y_target}:\")\n",
    "#             print(metrics)\n",
    "#             print(\"\\nBest parameters:\", grid_results['best_params'])\n",
    "#             print(\"Best CV score (RMSE):\", np.sqrt(grid_results['best_score']))\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {y_target}: {str(e)}\")\n",
    "#             continue\n",
    "    \n",
    "#     return results_dict\n",
    "\n",
    "# # Usage:\n",
    "# Y_columns = Y_labels_all\n",
    "# all_results = run_svr_for_multiple_targets(\n",
    "#     y_columns=Y_columns,\n",
    "#     Y_train=Y_train,\n",
    "#     Y_test=Y_test,\n",
    "#     X_train_transformed=X_train_yjs,\n",
    "#     X_test_transformed=X_test_yjs,\n",
    "#     Y_train_transformed=Y_train_yjs,\n",
    "#     Y_test_transformed=Y_test_yjs,\n",
    "#     preprocess_pipeline_Y=preprocess_pipeline_Y\n",
    "# )\n",
    "\n",
    "# # Create summary DataFrame with best parameters\n",
    "# summary_dict = {\n",
    "#     target: {\n",
    "#         **metrics.to_dict(),\n",
    "#         **{'best_' + k: v for k, v in grid_results['best_params'].items()}\n",
    "#     }\n",
    "#     for target, (metrics, grid_results) in all_results.items()\n",
    "# }\n",
    "\n",
    "# summary_df = pd.DataFrame.from_dict(summary_dict, orient='index')\n",
    "# print(\"\\nOverall Summary:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the grid search using the subset of Y labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_columns = merged_cols\n",
    "# all_results = run_svr_for_multiple_targets(\n",
    "#     y_columns=Y_columns,\n",
    "#     Y_train=Y_train,\n",
    "#     Y_test=Y_test,\n",
    "#     X_train_transformed=X_train_yjs,\n",
    "#     X_test_transformed=X_test_yjs,\n",
    "#     Y_train_transformed=Y_train_yjs,\n",
    "#     Y_test_transformed=Y_test_yjs,\n",
    "#     preprocess_pipeline_Y=preprocess_pipeline_Y\n",
    "# )\n",
    "\n",
    "# # Create summary DataFrame with best parameters\n",
    "# summary_dict_ss = {\n",
    "#     target: {\n",
    "#         **metrics.to_dict(),\n",
    "#         **{'best_' + k: v for k, v in grid_results['best_params'].items()}\n",
    "#     }\n",
    "#     for target, (metrics, grid_results) in all_results.items()\n",
    "# }\n",
    "\n",
    "# summary_df_ss = pd.DataFrame.from_dict(summary_dict_ss, orient='index')\n",
    "# print(\"\\nOverall Summary:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(summary_df_ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svectors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
